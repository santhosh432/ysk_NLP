{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nword2vec\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "word2vec\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
    "\n",
    "    size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "    window: (default 5) The maximum distance between a target word and words around the target word.\n",
    "    min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "    workers: (default 3) The number of threads to use while training.\n",
    "    sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation += '“”’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~“““”“”’'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'The University’s basic philosophy and goals find eloquent expression in its Coat of Arms emblazoning the motto “Tejaswinavadhitamastu” which in essence means “may the wisdom accrued deify us both the teacher and the taught and percolate to the Universe in its totality”, which in essence means “may learning illumine us both” (the teacher and the taught). '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = \"The earth revolves around the sun. The moon revolves around the earth\"\n",
    "\n",
    "def preprocessing(corpus): \n",
    "#     stop_words = set(stopwords.words('english'))     \n",
    "    training_data = [] \n",
    "    sentences = corpus.split(\".\") \n",
    "    for i in range(len(sentences)): \n",
    "        sentences[i] = sentences[i].strip() \n",
    "        sentence = sentences[i].split() \n",
    "        x = [word.strip(string.punctuation) for word in sentence \n",
    "                                     ] \n",
    "        x = [word.lower() for word in x] \n",
    "        training_data.append(x) \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_for_training(sentences): \n",
    "    data = {} \n",
    "    for sentence in sentences: \n",
    "        for word in sentence: \n",
    "            if word not in data: \n",
    "                data[word] = 1\n",
    "            else: \n",
    "                data[word] += 1\n",
    "    V = len(data) \n",
    "    data = sorted(list(data.keys())) \n",
    "#     print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data_for_training(preprocessing(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=preprocessing(corpus),size=10, window=2, min_count=1, iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=34, size=10, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taught',\n",
       " 'coat',\n",
       " 'both',\n",
       " 'goals',\n",
       " 'of',\n",
       " 'which',\n",
       " 'accrued',\n",
       " 'and',\n",
       " 'in',\n",
       " 'means',\n",
       " 'tejaswinavadhitamastu',\n",
       " 'may',\n",
       " 'expression',\n",
       " 'to',\n",
       " 'universe',\n",
       " 'university’s',\n",
       " 'totality',\n",
       " 'eloquent',\n",
       " 'the',\n",
       " 'percolate',\n",
       " 'us',\n",
       " 'deify',\n",
       " 'learning',\n",
       " 'emblazoning',\n",
       " 'philosophy',\n",
       " 'essence',\n",
       " 'find',\n",
       " 'basic',\n",
       " 'arms',\n",
       " 'its',\n",
       " 'motto',\n",
       " 'wisdom',\n",
       " 'teacher',\n",
       " 'illumine']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8785495,  2.0773106, -1.039631 , -1.0260286, -0.4412276,\n",
       "       -1.5850079,  1.4036156,  0.6081754, -0.9694794, -0.5724036],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['motto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tejaswinavadhitamastu', 0.9665277600288391),\n",
       " ('emblazoning', 0.9526838064193726),\n",
       " ('arms', 0.8971216082572937),\n",
       " ('of', 0.859471321105957),\n",
       " ('coat', 0.779770016670227),\n",
       " ('essence', 0.6904228925704956),\n",
       " ('which', 0.6825933456420898),\n",
       " ('wisdom', 0.6682392358779907),\n",
       " ('means', 0.6443667411804199),\n",
       " ('expression', 0.6222198009490967)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('motto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('totality', 0.9004559516906738),\n",
       " ('percolate', 0.8295577764511108),\n",
       " ('to', 0.7677906155586243),\n",
       " ('taught', 0.7211443781852722),\n",
       " ('teacher', 0.7040170431137085),\n",
       " ('learning', 0.6791481971740723),\n",
       " ('illumine', 0.6407046318054199),\n",
       " ('both', 0.572934627532959),\n",
       " ('us', 0.42882025241851807),\n",
       " ('essence', 0.42689037322998047)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('universe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(sentences=preprocessing(corpus), total_examples=6, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('motto', 0.9665277600288391),\n",
       " ('emblazoning', 0.8981133103370667),\n",
       " ('arms', 0.8514161705970764),\n",
       " ('of', 0.7871396541595459),\n",
       " ('wisdom', 0.7102418541908264),\n",
       " ('coat', 0.6970789432525635),\n",
       " ('means', 0.6797359585762024),\n",
       " ('essence', 0.673831045627594),\n",
       " ('which', 0.6660922765731812),\n",
       " ('may', 0.5758653879165649)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('tejaswinavadhitamastu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
